scenario_1:
  jobs:
    pysparkJob:
      mainPythonFileUri: gs://polidea-dataproc-utils/bigbench/trigger_bigbench_benchmark.py
      args:
        - gs://polidea-dataproc-utils/bigbench
        - scenario_1
        - -f 1
    stepId: a111
  placement:
    managedCluster:
      config:
        gceClusterConfig:
          metadata:
            file_handle_limit: "10000000"
            swap_size: 5G
        masterConfig:
          minCpuPlatform: "Intel Ivy Bridge"
        workerConfig:
          numInstances: 10
        initializationActions:
          executableFile: gs://polidea-dataproc-utils/bigbench/bigbench.sh

scenario_2:
  jobs:
    pysparkJob:
      mainPythonFileUri: gs://polidea-dataproc-utils/bigbench/trigger_bigbench_benchmark.py
      args:
        - gs://polidea-dataproc-utils/bigbench
        - scenario_1
        - -f 1
    stepId: a111
  placement:
    managedCluster:
      config:
        masterConfig:
          minCpuPlatform: "Intel Ivy Bridge"
        workerConfig:
          numInstances: 2
        softwareConfig:
          properties:
            mapred:mapreduce.map.cpu.vcores: '1'
            yarn:yarn.app.mapreduce.am.resource.mb: '2048'
        initializationActions:
          executableFile: gs://polidea-dataproc-utils/bigbench/bigbench.sh
